* [paper](paper/13.206-18-CCNet-Criss-Cross-Attention-for-Semantic-Segmentation.pdf)

## how

* 下图蓝色点为待处理像素点 ![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D) ,处理后得到结果为红色点 ![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D) :

  ![img](readme/13.206-网络结构.png)

（a）是Non-local，含蓝色中心的feature map是输入，分为上下两个分支处理：

深绿色分支代表已经完成Non-local操作，得到了 ![f(x_{i},x_{j})](https://www.zhihu.com/equation?tex=f%28x_%7Bi%7D%2Cx_%7Bj%7D%29) (绿色的深浅则代表了当前位置与蓝色中心点的相关性大小)；

下面灰色分支代表进行了 ![g(x_{j})](https://www.zhihu.com/equation?tex=g%28x_%7Bj%7D%29) 操作。将两个结果相乘，得到 ![y_i](https://www.zhihu.com/equation?tex=y_i) (含红色中心的feature map).

（b）即为CCNet的改进，可以看到，深绿色 ![f](https://www.zhihu.com/equation?tex=f) 部分是十字型结构，意即只计算当前 ![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D) 周围十字型区域像素 ![x_{j}](https://www.zhihu.com/equation?tex=x_%7Bj%7D) 与它的相关性。当然，我们需要知道是 所有像素 与 ![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D) 的相关性，于是作者将这个过程进行堆叠，并且通过实验发现，只需堆叠两次即可覆盖所有点，并超越non-local的效果。



为什么堆叠两次即可？

我们先看看信息是如何通过十字型结构传递的：

![img](readme/13.206-信息传递.png)

上图展示了蓝色像素点的信息传达到左下角点的过程：

第一次loop1，我们在计算左下角点的f时，只能包含左上角点和右下角点的信息，此时并没有左下角点与蓝色点的相关信息。

但在计算左上（or右下）点时，是计算了蓝色点与它们的相关性信息的。

第二次loop2，当我们再次计算左下角点的 ![f](https://www.zhihu.com/equation?tex=f) 时，再次包含左上&右下点的信息，此时的左上&右下已经不是当初那个它们了，它们已经有了蓝色点的信息，此时便可以间接地将蓝色点信息传递给左下点。

同理，其他不在左下点十字型位置的像素点，都可以通过这种方式在第二次loop的时候就将信息传递给左下点。于是实现两次loop便“遍历”了所有点。

事实上，我们可以发现蓝色点信息是传递了两遍给左下点的（左上传递了一次，右下传递了一次），虽然是间接传递没有直接计算得到的结果强度大，但这种对于信息的两次加强也很有可能是最终效果 略胜于 Non-local的原因之一。

于是乎，原本需要计算 ![(H*W)^{2}](https://www.zhihu.com/equation?tex=%28H%2AW%29%5E%7B2%7D) 次，现在变为了 ![(H*W)*(H+W-1)](https://www.zhihu.com/equation?tex=%28H%2AW%29%2A%28H%2BW-1%29) （一共 ![H*W](https://www.zhihu.com/equation?tex=H%2AW) 个像素点，每个点只"观照"其十字型区域 ![(H+W-1)](https://www.zhihu.com/equation?tex=%28H%2BW-1%29) 面积的像素）。计算效率大大提升！

以下是CCNet的网络结构：

![img](readme/13.206-网络结构-02.png)



## reference
* [CCNet--于"阡陌交通"处超越恺明Non-local](https://zhuanlan.zhihu.com/p/51393573)