* [2018-Dual Attention Network for Scene Segmentatio](paper/13.205-18-Dual-Attention-Network-for-Scene-Segmentation.pdf)

### 整体

![Dual_Attation_Network_框架](readme/13.205-框架.png)

* 我们提出DANet。使用自注意力机制来捕获视觉特征依赖于空间和通道维度。尤其是我们在传统FCN基础上添加并行的两个注意力模块，一个是位置注意力模块，它在特征图的任意两个位置捕获空间依赖关系。对于中间位置的特征，通过聚合所有位置的特征 加权求和，权值又两个位置的相关性决定，有相似特征的两个位置可以相互提高贡献不管他们的空间维度的距离。对于通道注意力模型，我们使用相似的自注意力机制来捕获通道依赖在两个通道图之间，并且通过加权和所有的通道特征来更新每一个通道图。最后融合两个模型的输出。                                                                                                                                                                                                                                                                                                                                                                                                                                                            
* 我们模型的优势，图一所示：
  1. 一些人和交通灯在第一行中是不显眼的或者不完全的目标。如果探索简单的上下文嵌入，来自主导的对象（例如汽车，建筑物）的上下文将无助于标记不显眼的对象。相反，在我们的注意力模型中，我们选择聚合不突出物体相似的语义特征来提高特征表示并且避免明显物体的影响。
  2. 物体尺度多样，识别这样的物体需要不同尺度的上下文信息。也就是说，不同尺度的特征不应该同等对待对于表示不同的语义来说，我们的注意力机制模型致力于适应整个相似的特征来自同一个场景的不同尺度。这样可以解决上面的问题。
  3. 我们现实利用空间关系和通道关系，因此场景理解可以受益于长返回依赖。

### 两个结构的分别介绍

![Dual_Attation_Network_子模块](readme/13.205-子模块.png)

* 我们将特征喂入空间长返回依赖通过下面三个步骤：
  1. 生成空间注意力矩阵，模型化空间关系任何像素间的，
  2. 我们使用矩阵相乘在注意力举证和原来特征图之间
  3. 在结果矩阵和原来特征图之间做一个像素级求和。
    同时，通道间长距离依赖被捕获通过通道注意力模型。

![Dual_Attation_Network_子模块添加后的效果图](readme/13.205-子模块添加后的效果图.png)

#### 位置注意力模块

![Dual_Attation_Network_位置注意力模块实现](readme/13.205-位置注意力模块实现.png)

#### 通道注意力模块

![Dual_Attation_Network_通道注意力模块实现](readme/13.205-通道注意力模块实现.png)