## who

* 2016 CVPR
* 作者：Deepak Pathak etc. 发表于2016年，可能是用对抗网络做 图像修补 (Image Completion/ Inpainting) 的第一篇吧
* [paper](paper/41.301-16-Context-Encoders--Feature-Learning-by-Inpainting.pdf)
* [Context Encoders 的阅读与翻译：用对抗网络修补图片中的缺失区域](<https://zhuanlan.zhihu.com/p/54070291>)

## what

* 我们提出了一种基于上下文的像素预测(context-based pixel prediction) 驱动的无监督的视觉特征学习(visual feature learning) 算法。
* 通过对自动编码器的类推，我们提出了**上下文编码器(Context Encoders)** ——一种在训练后能够根据周围环境为条件，生成任意图像区域的卷积神经网络。为了达成这项任务，上下文编码器既需要理解整张图像的内容，也需要为缺失的区域进行合理的假设，并生成缺失部分的内容 (produce a plausible hypotheisis)。
* 在训练上下文编码器的时候，我们已经尝试了标准的像素重建损失 ![\mathcal{L}_{rec}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Brec%7D) (pixel-wise reconstruction)，以及 像素重建损失 ![\mathcal{L}_{rec}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Brec%7D) 与对抗损失 ![\mathcal{L}_{adv}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Badv%7D) 相加后的和数。后者产生了更加清晰的结果，因为它可以更好地处理输出的多个模式(handle multiple modes in the output)。我们发现上下文编码器的表示学习，捕获到的不仅仅是视觉结构的外观，而且还有图片的语义信息。(context encoder learns a representation that captures not just appearance but also the semantics of visual structures)
* 我们定量地展示了我们为预训练的学习到的卷积神经网络特征 在分类、检测以及语义分割任务上面的有效性。此外，上下文编码器可以用于语义上的图像修复任务，也可以是独立的或者非参数方式的初始化。

## where

### 动机

* 我们眼中的世界是非常丰富的，同时也是高度结构化的，人类有着不可思议的能力，来理解这些结构。在这项工作中，我们将探索是否可以用最先进的计算机视觉算法来做到相同的事情。如图1a所示，尽管图片的中心部位已经缺失，但是大部分人可以轻松地根据它周围的像素，来想象出缺失部位的内容。我们中的某些人甚至可以把它画出来，就像图1b展示的那样。（我们人类能拥有）这种能力，源于这样的事实：尽管自然图像(nautral images) 是多样的，但是它们都是高度结构化的（例如墙体立面上的规则窗户图案）。我们人类可以理解这些结构，然后对视觉结果进行预测，甚至当我们只看到一部分场景的时候，我们也能这么做。

* 在这篇文章中，我们展示了使用卷积神经网络 学习对这些结构进行预测 是可行的，而卷积神经网络最近（文章写于2016）在各种图像理解任务中取得成功的一类模型。

![image-20190503142750046](readme/41.301-问题示意图-01.png)

* 给出一幅带有缺失区域的图像（例如图1a），我们训练了一个卷积神经网络 去复原这些缺失的像素值（如图1d）。我们把我们的模型称为 **上下文编码器(context encoder)**。它由一个捕获图片上下文并将图片转化为紧凑的潜在表示(compact latent feature representation)的编码器，以及一个生成缺失图片内容的解码器。上下文编码器与自动编码器密切相关，它们都有一个相似的 编码-解码 结构。自动编码器接受一张图片，然后在它经过低维度的瓶颈层的时候，将其重构为包含了可以表示场景的压缩（语义）特征。不幸的是，这些特征表示似乎只是将图片内容压缩，而并没有（真正地）学习到有意义的语义表示。[去噪自动编码器(Denoising autoencoders)](https://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.569.2442%26rep%3Drep1%26type%3Dpdf) 通过破坏(corrupting)输入的图片来解决这个问题（可以是删去图中的一些像素，类似于dropout/partially destroyed），要求网络可以撤销这些（对图片的）损伤。然而，这些对图像的破坏方式是通常过于局部，过于低级(localized and low-level)，完成这些撤销并不需要过多的语义信息。

### 创新

* 相比之下，我们的上下文编码器需要解决更艰巨的任务：填充图片中的大面积缺失区域，它无法从临近的区域获得用来“提示 hints”的像素。这需要对场景有更加深刻的语义信息理解，与在更大的空间范围内，合成更高层级特征的能力。例如，在图1a中，它需要“凭空捏造(out of the thin air)”出一个完整的窗户。这使得它与 学习从自然语言中，根据上下文预测单词的word2vec 有些神似。

> 所以作者Deepak Pathak他们 才将这个基于autoencoders 的模型命名为 context encoder吧。work2vec 是根据上下文填补缺失的单词，context encoder 是根据周围的环境(context)，填补缺失的图片区域

* 就如自动编码器一样，上下文编码器通过完全无监督的方式进行训练。我们的结果表明：为了达成这项任务，模型需要在理解图片上下文的同时，也做出对图片缺失部位的合理假设。然而对于这个任务，在保持（图片）连贯性的基础上，还存在着多种填补缺失区域的方式。（为了尽量避免这种情况），我们通过联合训练我们的上下文编码器来最小化重建损失 ![\mathcal{L}_{rec}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Brec%7D) 与对抗损失 ![\mathcal{L}_{adv}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Badv%7D)，来将**这种麻烦(burden)** 与我们的损失函数分开。

> 关于**这种麻烦(burden)**：如下图，白色区域缺失后的左图，我有右图上下两种修复方式，那么，右图的修复结果都相当逼真，那么那一张是我们更希望得到的结果呢？这就是保持图片连贯性，对缺失区域的填补有多种填补方式带来的矛盾。解决方式当然是在对抗损失 ![\mathcal{L}_{adv}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Badv%7D) 的基础上，再加上一个重建损失 ![\mathcal{L}_{rec}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Brec%7D)，当恢复得到的图片逼真连贯时，对抗损失 ![\mathcal{L}_{adv}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Badv%7D)会减少，当恢复区域与原先区域相似的时候，重建损失 ![\mathcal{L}_{rec}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Brec%7D)会减小。

![image-20190503143503181](readme/41.301-burden示意图-01.png)



* 重建损失（L2）捕获了与上下文相关的缺失区域 的整体结构，而对抗损失具有 从分布中选择特定（填补）模式 的效果。如图1所示，值使用重构损失将会产生模糊的结果。然而添加了对抗损失后，产生了更加清晰的预测结果。

* 我们独立地评估了编码器与解码器。
  * 对于编码器，我们展示了：仅仅通过对图片补丁上下文的编码，以及使用特征提取的结果，就能够从数据集中检索临近的上下文信息，产生在语义上类似于（没见过的）原始补丁。我们通过微调编码器，来进一步用具特征学习的质量，以用于各种图像理解任务，包括分类、对象检测、语义分割。我们与目前最好的无监督、自监督方法，在相同的任务上面比较。
  * 对于解码器，我们展示了我们的方法在通常情况下，能够填充逼真的图像内容。事实上，据我们所知，在孔洞修补任务上（大区域的缺失修补），我们是第一个能够给出合理的语义信息修复结果 的参数化的图像修复算法(parametric inpatinting algorithm)。在无参数的图像修补方法中，上下文编码器也可以被用来计算更好视觉特征。

### 相关工作

* 过去的十年中，计算机视觉在图片的语义信息理解任务上已经取得了巨大的成果，例如分类、目标检测、语义分割。最近的卷积网络模型在这些任务上表现了巨大的性能优势（这里的性能performance 更多指的是输出结果的质量，而不是硬件运行速度）。这个模型在图片分类任务上面取得的成功，为解决更加困难的问题铺平了道路，包括无监督的情况下，理解并生成自然图片。我们简要地回顾与这篇文章相关的子领域的一些工作。

* **无监督学习 Unsupervised learning** 用上百万张ImageNet中的图片，训练得到卷积网络分类模型，学习到的特征，在交叉任务上面的泛化性能非常地好。然而，这种语义信息和泛化特征是否可以单独地从原始图片中学习到，仍然是一个没有解决的问题。无监督的深度学习中，最早的工作是自动编码器。遵循着相同的思路，噪自动编码器从局部破坏的图片中 重构出原来的图片，使得编码的时候，面对破坏依然可以保持稳健性(robust)。在此处，上下文编码器可以被认为是去噪自动编码器的一种变体，而传递给上下文编码器的输入图片损坏的空间要更大，因此它需要要更多的语义信息完成（对损坏的）撤销。
* **弱监督与自监督学习 Weakly-supervised and self-supervised learning** 不久前，人们对通过弱监督和自监督来自我学习产生了浓厚的兴趣。一种有用的监督学习资源是使用视频中包含的时态信息。跨帧连续性(consistency across temporal frames) 在嵌入学习(learn embeddings) 中已经的得到使用，并在几个任务上取得良好的表现。另一种使用连续性的方法是 追踪包含了相关任务属性的视频中的补丁，并使用追踪补丁的连贯性来指导训练 。从非视觉传感器上读取出自我运动(Ego-motion) 已经被用作训练视觉特征的监督信号

* 与本文最密切相关的是利用空间的上下文背景 作为自由且丰富的监督信号来源。visual Memex([Menory-Extender](https://link.zhihu.com/?target=https%3A//www.baidu.com/link%3Furl%3DIAFcHowkJ2iNM8eUtvoFvKGMVW0RV2BQPvJmRCjNO7fGLMhTcL5mrawT60f1M8go%26wd%3D%26eqid%3Df2e881ca0000c812000000035c30644a)) 将上下文用于非参数模型的对象关系 和预测掩膜对象(predict masked objects) 的场景中，从而使用上下文建立起对无监督对象的发现（体系）。然而，两个方法都依赖于手工设计的特征，并且在表示学习中表现得不好。

* 最近，**Doersch等人** 在 图片中的相邻方块预测相对位置的任务中 训练无监督深度特征表示。我们与Doersch等人的研究目标相同，但是在方法上面有着根本的不同：虽然都是解决一个识别的任务（补丁A 是优于 还是劣于 补丁B）我们的上下文编码器解决了一个纯粹的预测问题（即缺损区域中的像素应该是什么值？）

* 有趣的是，在使用语言的上下文学习单词嵌入的任务中，也有着相同的区别：Collobert 提倡一种拼接方式，而在 word2vec 模型中将其标书为单词预测。

* 我们的方法有一个重要的好处，就是我们的监督信号更加的丰富：在每个训练例子中，一个上下文编码器需要预测大约15,000张真实图片，而在Doersch 的方法中的8个选择只有1个选项。可能由于这种差异，我们的上下文编码器需要的时间远远少于 Doersch 的方法。此外基于上下文的预测也更难以“投机取巧/作弊 (cheat)”，因为诸如色差之类的低级图像特征，不能提供任何有意义的信息。这一点与 Doersch 中 根据色差（提供的信息）部分地解决任务 形成对比。另一方面，尚不清楚是否需要可信的像素数据来生成良好的视觉特征。

## how

###  **用上下文编码器进行图像生成**

* 我们现在介绍上下文编码器：使用卷积网络，从周围的环境 预测场景中的缺失区域。首先，我们对模型的基本结构进行概述，然后提供学习过程的详细信息，最后介绍图像区域的各种移除策略。

### 编码-解码流程

* 模型的整体架构是一个简单的编码-解码器。编码器接受部分区域缺失的图片作为输入，然后输出潜在的特征 用于表示这幅图片。解码器接受这些表示特征，然后输出缺失的图片区域。我们发现：在编码器与解码器之间，用全连接层连接起来是重要的（全连接层的节点数量与通道数量相当），它允许每个解码器单元对整张图片进行推断。图2展示了我们架构的概观。

#### 1. 编码器

* 我们的编码器是中AlexNet架构里面分离出来的。输入图片的尺寸是227x227，我们使用了前五个卷积层，和跟在它们后面的池化层（称为*pool5*），计算得到一个6x6x256的特征表示张量。与AlexNet相对比，我们的模型不需要在ImageNet上面进行分类训练。相反，网络被用来进行“从零开始(from scratch) ”的上下文预测训练。

* 然而，如果仅限于使用卷积层作为编码器的架构，那么没有任何方法 可以将获取到的信息从特征映射的一角传播到另外一角。这是由于卷积网络（只做到了）将所有特征映射连接在一起，但却没有直接将所有的位置用一个特定的特征映射连接起来。目前的这个架构中，这一类信息的传播完全由全连接层或者隐含层处理，所有的激活函数都直接与每个（节点）直接连接。在我们的架构中，潜在特征的维度是 6 × 6× 256 == 9216，对于编码器和解码器都是这样的。

* 与自动编码器不同，我们没有对原始的输入进行重构，（因此我们将它们设计成全连接层，）是因为我们没有想自动编码器一样拥有一个比较小的瓶颈层(*bottleneck*) 。然而，用全连接层完全连接编码器和解码器会导致参数爆炸（超过100M）。在某种程度上，对当前的GPU进行高效的训练是困难的。为了缓解这个问题，我们使用与特征通道相同宽度的全连接层缓解这个问题，详情如下。

#### 2. 与通道等宽的全连接层 Channel-wise fully-connected layer

* 这一层实质上是一个全连接层分组，用于传递每一个特征映射中激活后的信息。如果输入层有 m 个n×n 特征映射，那么这一次将会输出维度相同的 m 个n×n 特征映射。然而，与于全连接层不同，它没有连接不同的特征映射，而是在特征映射之间传播信息。因此，等宽通道全连接层的参数数量为 ![mn^4](https://www.zhihu.com/equation?tex=mn%5E4) ，比起 ![m^2n^4](https://www.zhihu.com/equation?tex=m%5E2n%5E4) 的参数数量已经减少不小了（忽略偏置项）。接下来是一个步长为1 的卷积模块执行跨通道的信息传播任务。

* 解码器 Decoder 我们现在讨论流程图中的下半部分，使用编码特征生成图片像素的解码器。“编码特征”使用上面提及的**通道等宽全连接层**与“解码特征”相连接。

* 等到等宽全连接层 后面连接了 5个 上采样卷积层(up-convolutional) （它似乎就是**转置卷积**层，文章发表于2016，翻译写于2018），每层由包含了ReLU作为激活函数。上采样卷积(up-convolutional) 可以简单地描述为一种卷积，它能够输出更高分辨率的图像。它可以理解成是像上采样一样的卷积，或者一种步长是分数的卷积方式。在这项技术背后的应用是直观的 (The intuition behind this is straightforward) ，这一系列的上采样卷积，加上非线性加权的引入，对特征映射进行上采样，直到图片大致达到原始目标的尺寸。

###   损失函数

* 我们通过恢复图片中的缺失部分训练我们的上下文编码器。然而，对于缺失的区域，经常会有许多种似是而非的填充结果。我们通过解耦合联合函数(decoupled joint loss function) 来模拟这种行为，用以处理上下文的连续性 以及多种输出模式。
  * 一方面，重构损失(L2) 负责刻画缺失区域整体结构 以及 缺失区域与背景的一致性。但是重构损失倾向于平均输出 多种填充模式的平均值。
  * 另一方面，对抗损失试图让预测结果看起来真实，并且它具有从分布中，选择出特定填充模式的效应。对于每个真实图像数据 ![x](https://www.zhihu.com/equation?tex=x) ，我们的上下文编码器 ![F](https://www.zhihu.com/equation?tex=F) 会产生输出 ![F(x)](https://www.zhihu.com/equation?tex=F%28x%29) 。使得 ![\hat{M}](https://www.zhihu.com/equation?tex=%5Chat%7BM%7D) 成为一张代表缺失区域的二值的掩膜层(mask)（像素数值为1的区域表示丢弃，为0的区域表示输入）。在训练期间，这些掩膜层在每一轮训练中都是随机生成的，就像在第3.3节描述的一样。

我们现在开始叙述损失函数的不同组成成分。

**重构损失 Reconstruction Loss** 我们使用掩膜层下的 ![L2](https://www.zhihu.com/equation?tex=L2) 距离作为我们的损失函数 ![\mathcal{L}_{rec}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Brec%7D)

![\mathcal{L}_{rec} =  || \hat{M} \odot (x - F \big((1 - \hat{M}) \odot x \big) ) ||_2](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Brec%7D+%3D++%7C%7C+%5Chat%7BM%7D+%5Codot+%28x+-+F+%5Cbig%28%281+-+%5Chat%7BM%7D%29+%5Codot+x+%5Cbig%29+%29+%7C%7C_2)

这里的符号 ![\odot](https://www.zhihu.com/equation?tex=%5Codot) 是矩阵对应位置相乘(the element-wise product operation) 。我们尝试了 ![L1](https://www.zhihu.com/equation?tex=L1) 与 ![L2](https://www.zhihu.com/equation?tex=L2) 距离，发现它们没有显著的差异。虽然这种简单的损失促使解码器产生预测对象的初略轮廓，但它通常无法捕获任何的高频率出现的细节（见图1c）。这源于 ![L2](https://www.zhihu.com/equation?tex=L2) （或者![L1](https://www.zhihu.com/equation?tex=L1)）距离损失通常比输出高度精确的纹理更喜欢输出模糊的解决方案。我们认为这是因为在使用L2损失的情况下， **预测缺失值的平均分布是更加“安全”的做法**。因为平均预测像素值的方式，能够使误差最小化，但是这样导致生成的是模糊的预测平均图片。我们可以通过增加对抗性损失解决这个问题。

**对抗损失 Adversarial Loss** 我们的对抗损失基于 生成对抗网络(GAN) 。为了从数据分布中学习生成模型G，GAN 提议 将 生成模型G 需要与 对抗判别模型D联合在一起学习，以便判别器为生成器提供损失梯度。 ![G: \mathcal{Z} → \mathcal{X}](https://www.zhihu.com/equation?tex=G%3A+%5Cmathcal%7BZ%7D+%E2%86%92+%5Cmathcal%7BX%7D) 映射G将 噪声分布Z映射到 数据分布X上。这中学习 是一个双人博弈的过程，其中判别器D 接受生成器G 的生成结果，然后输出对生成结果预测，试图将生成结果与真实结果区分出来。生成器G 试图 通过产生与真实样本尽可能相似的图片，混淆判别器D。判别器的优化目标是 输出 指示输入图片是否为真实样本 的逻辑似然性(logistic likehood)：

![\min \limits_G  \max \limits_D \  \mathbb{E}_{x \in \mathcal{X}}  [\log(D(x))] +  \mathbb{E}_{z \in \mathcal{Z}}  [\log(1 - D(z))] ](https://www.zhihu.com/equation?tex=%5Cmin+%5Climits_G++%5Cmax+%5Climits_D+%5C++%5Cmathbb%7BE%7D_%7Bx+%5Cin+%5Cmathcal%7BX%7D%7D++%5B%5Clog%28D%28x%29%29%5D+%2B++%5Cmathbb%7BE%7D_%7Bz+%5Cin+%5Cmathcal%7BZ%7D%7D++%5B%5Clog%281+-+D%28z%29%29%5D+)

这个方法最近在图像生成建模中，展现出令人鼓舞的结果。因此，我们在上下文编码器中采用了这个框架进行上下文预测。例如![G \triangleq F](https://www.zhihu.com/equation?tex=G+%5Ctriangleq+F) （这是什么意思？）。需要为这个任务自行定义一个GANs，它可以根据给出的上下文信息为条件（例如掩膜层 ![\hat{M} \odot x](https://www.zhihu.com/equation?tex=%5Chat%7BM%7D+%5Codot+x) ）。



然而，条件GANs 在上下文预测任务上 不容易训练。因为对抗判别器D容易利用生成区域中的不连续感知(exploits the perceptual discontinuity) 和原始的上下文图片，轻易地对预测样本以及实际样本进行分类。因为我们通过在上下文编码器中调整生成器，替代原先的公式。我们还发现，当生成器没有以噪声矢量z为输入条件时，生成结果得到改善。于是，我们的上下文编码器中的GAN对象 ![\mathcal{L}_{adv}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Badv%7D) 如下所述：

![\mathcal{L}_{adv} =  \max \limits_{D} \  \mathbb{E}_{x \in \mathcal{X}} [ \log (D(x)) + \log(1-  D(F((1-\hat{M}) \odot x)))]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Badv%7D+%3D++%5Cmax+%5Climits_%7BD%7D+%5C++%5Cmathbb%7BE%7D_%7Bx+%5Cin+%5Cmathcal%7BX%7D%7D+%5B+%5Clog+%28D%28x%29%29+%2B+%5Clog%281-++D%28F%28%281-%5Chat%7BM%7D%29+%5Codot+x%29%29%29%5D)

其中，在实际训练时，函数F与D联合使用SGD（随机梯度下降）进行优化。注意虽然这个优化对象促使上下文编码器的输出看起来逼真，但是它的作用范围不仅仅是等式中的缺失区域



**联合损失 Joint Loss** 我们将整体的损失函数定义为：

![\mathcal{L} =  \lambda_{rec} \mathcal{L}_{rec} + \lambda_{adv} \mathcal{L}_{adv} \\](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D++%5Clambda_%7Brec%7D+%5Cmathcal%7BL%7D_%7Brec%7D+%2B+%5Clambda_%7Badv%7D+%5Cmathcal%7BL%7D_%7Badv%7D+%5C%5C)

目前，我们使用的对抗损失仅用于图像修补实现。因为AlexNet 架构的训练与联合对抗损失不兼容。详情见第5.1, 5.2 节。











